{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Captioning.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqCatvZB3CHIxTudBOx/OA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amifunny/Deep-Learning-Notebook/blob/master/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLBjzEH3R06j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print( tf.__version__ )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LOAhKOWI9lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Image Captioning - Based on End to End Modeling\n",
        "\n",
        "  See Paper - 'https://arxiv.org/pdf/1411.4555.pdf'\n",
        "\n",
        "  Encoder - PreTrained Imagenet ResNet50 Model\n",
        "  Decoder - LSTM with Dimension 512\n",
        "\n",
        "  Sampling Via Beam Search\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDrcjfq67AtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "from collections import Counter\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upnmDLJ9I8BP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget 'http://images.cocodataset.org/zips/train2014.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPr-ZRPMRscK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!unzip 'train2014.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDaDO0a3Txf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWaZSxK8T1bL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip 'annotations_trainval2014.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6mjv65qdsea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc_dim = 512\n",
        "hidden_dim = 512\n",
        "\n",
        "batch_size = 32\n",
        "img_size = 224\n",
        "vocab_size = 10000\n",
        "\n",
        "embed_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FDw5_CAEJLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is to find vocab_size\n",
        "# Again Reading the file redundant , but lets do it\n",
        "annotation_file = '/content/annotations/captions_train2014.json'\n",
        "with open(annotation_file) as file:\n",
        "\n",
        "  content = file.read()\n",
        "  file.close()\n",
        "\n",
        "annotate_dict = json.loads(content)\n",
        "\n",
        "all_words = []\n",
        "\n",
        "for idx in range( len(annotate_dict['annotations']) ):\n",
        "\n",
        "  img_caption = annotate_dict['annotations'][idx]['caption']\n",
        "\n",
        "  cleaned_caps = re.sub( r'[^a-zA-Z0-9. ]' , '' , img_caption )\n",
        "  cleaned_caps = re.sub( r'[.]' , ' .', cleaned_caps )\n",
        "\n",
        "  capt_words = cleaned_caps.lower().split()\n",
        "  all_words.extend( capt_words )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukWLKyHcdoiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_int = { '<pad>':0,'<start>':1,'<end>':2,'<unk>':3 }\n",
        "int_to_word = { 0:'<pad>',1:'<start>',2:'<end>',3:'<unk>' }\n",
        "before_size = len( word_to_int )\n",
        "\n",
        "occur_list = Counter(all_words)\n",
        "unique_list = occur_list.most_common()\n",
        "vocab_words =  unique_list[:vocab_size-len(word_to_int)]\n",
        "\n",
        "for i in range( len(vocab_words) ):\n",
        "\n",
        "  word_to_int[ vocab_words[i][0] ] = i+before_size\n",
        "  int_to_word[ i+before_size ] = vocab_words[i][0]\n",
        "\n",
        "print( len(word_to_int) )\n",
        "print( len(int_to_word) )\n",
        "print( int_to_word )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdWUpql3RpWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_shuffled_annots(start=0,limited_num=1024):\n",
        "  \n",
        "  annotation_file = '/content/annotations/captions_train2014.json'\n",
        "  with open(annotation_file) as file:\n",
        "\n",
        "    content = file.read()\n",
        "    file.close()\n",
        "\n",
        "  annotate_dict = json.loads(content)\n",
        "  \n",
        "  content_list = annotate_dict['annotations'][start:start+limited_num]\n",
        "\n",
        "  # shuffle image annotions and hence the images\n",
        "  random.shuffle( content_list ) \n",
        "\n",
        "  return content_list\n",
        "\n",
        "def get_data( content_list ,start=0,noi=100):\n",
        "\n",
        "  images = []\n",
        "  images_caption = []\n",
        "\n",
        "  for idx in range( start , start + len(content_list[start:start+noi]) ):\n",
        "\n",
        "    img_id = content_list[idx]['image_id']\n",
        "    img_caption = content_list[idx]['caption']\n",
        "    img_name = 'COCO_train2014_'+\"{:012d}\".format(img_id)+\".jpg\"\n",
        "\n",
        "    img = Image.open( '/content/train2014/'+img_name )\n",
        "    img_arr = np.array( img.resize( [img_size,img_size] ) )/127.5\n",
        "    img_arr = img_arr-1.0\n",
        "    if img_arr.shape[-1]!=3:\n",
        "      img_arr = np.repeat( img_arr[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "\n",
        "    cleaned_caps = re.sub( r'[^a-zA-Z0-9. ]' , '' , img_caption )\n",
        "    cleaned_caps = re.sub( r'[.]' , ' .', cleaned_caps )\n",
        "\n",
        "    splited_caps = [int_to_word[1]] + cleaned_caps.lower().split() + [int_to_word[2]]\n",
        "\n",
        "    images.append( img_arr )\n",
        "    images_caption.append( splited_caps )\n",
        "\n",
        "  return images,images_caption"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks_R0YzbeOOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annot_list = get_shuffled_annots()\n",
        "# *******\n",
        "total_examples = len( annot_list )\n",
        "print( \"Total Number of Examples ==>  {}\".format(total_examples) )\n",
        "# *******\n",
        "\n",
        "\n",
        "images,images_caption = get_data( annot_list , start=0,noi=256)\n",
        "print( len(images) )\n",
        "print( len(images_caption) )\n",
        "\n",
        "for i in range(3):\n",
        "  print(images_caption[i])\n",
        "  print(images[i].shape)\n",
        "  show = plt.imshow( images[i] )\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01bhzN0nFmuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_int(word_captions):\n",
        "\n",
        "  int_cap = []\n",
        "\n",
        "  for each_cap in word_captions:\n",
        "\n",
        "    int_each_cap=[]\n",
        "    for w in each_cap:\n",
        "\n",
        "      int_each_cap.append( word_to_int.get( w , word_to_int['<unk>'] ) )\n",
        "\n",
        "    int_cap.append(int_each_cap)\n",
        "\n",
        "  return int_cap  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU7wCV6BGgB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_word(int_captions):\n",
        "\n",
        "  word_cap = []\n",
        "\n",
        "  for each_cap in int_captions:\n",
        "\n",
        "    word_each_cap=[]\n",
        "    for i in each_cap:\n",
        "\n",
        "      word_each_cap.append( int_to_word[i] )\n",
        "\n",
        "    word_cap.append(word_each_cap)\n",
        "\n",
        "  return word_cap  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhPTOhoVnvCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "   \n",
        "img_batches = []\n",
        "cap_batches = []\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def prep_batch(imgs,caps,batch_size):\n",
        "\n",
        "    num_of_batches = int(len(imgs)/batch_size)\n",
        "    \n",
        "    img_batches = []\n",
        "    cap_batches = []\n",
        "    \n",
        "    for i in range(num_of_batches):\n",
        "\n",
        "      img_batch = tf.convert_to_tensor( imgs[i*batch_size:(i+1)*batch_size] , tf.float32 )\n",
        "      img_batches.append( img_batch )\n",
        "\n",
        "      padded_batch = tf.keras.preprocessing.sequence.pad_sequences( caps[i*batch_size:(i+1)*batch_size] ,padding='post')\n",
        "      cap_batch = tf.convert_to_tensor(padded_batch)\n",
        "      cap_batches.append( cap_batch )\n",
        "\n",
        "    return img_batches,cap_batches,num_of_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4-xaMXaNiRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_caps = convert_to_int( images_caption )\n",
        "img_batches,cap_batches,num_of_batches = prep_batch( images , int_caps , 32 )\n",
        "\n",
        "print( img_batches[0].shape )\n",
        "print( cap_batches[0].shape )\n",
        "print( num_of_batches )\n",
        "\n",
        "for batch in cap_batches:\n",
        "  # int caption batch\n",
        "  print( batch )\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_XK4vN9OQAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet = tf.keras.applications.InceptionV3( include_top=False , input_shape=[img_size,img_size,3] )\n",
        "resnet.trainable = True\n",
        "\n",
        "inputs = tf.keras.layers.Input( shape=[img_size,img_size,3] )\n",
        "out = resnet( inputs )\n",
        "out = tf.keras.layers.GlobalAveragePooling2D()(out)\n",
        "out = tf.keras.layers.Dense( hidden_dim*2 ,activation='relu')(out)\n",
        "outputs = tf.keras.layers.Dense( hidden_dim ,activation='relu' )(out)\n",
        "\n",
        "encoder_model = tf.keras.Model( inputs , outputs )\n",
        "encoder_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYlzNb_fOuJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Single Stacked LSTM\n",
        "\n",
        "  hidden_state of LSTM same as output of encoder\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "inputs = tf.keras.layers.Input([None])\n",
        "hidden_state_in = tf.keras.layers.Input([enc_dim])\n",
        "\n",
        "embed_out = tf.keras.layers.Embedding( vocab_size , embed_dim )(inputs)\n",
        "\n",
        "out,hidden_state_out = tf.keras.layers.GRU( hidden_dim , return_state=True ,return_sequences=True )(embed_out,initial_state=[hidden_state_in])\n",
        "out = tf.keras.layers.Dense( hidden_dim*2 )(out)\n",
        "outputs = tf.keras.layers.Dense( vocab_size , activation='softmax' )(out)\n",
        "\n",
        "decoder_model = 0\n",
        "decoder_model = tf.keras.Model( [inputs,hidden_state_in] , [outputs,hidden_state_out] )\n",
        "decoder_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxnjEy7BQ4zM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#********************\n",
        "batch_size = 32\n",
        "l_rate = 0.0001\n",
        "ctr=0\n",
        "epochs=10\n",
        "subset_size = 2048\n",
        "step_limit = 25\n",
        "#********************\n",
        "\n",
        "mean_loss = tf.keras.metrics.Mean()\n",
        "optimizer = tf.keras.optimizers.RMSprop( l_rate )\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  for e in range(epochs):\n",
        "\n",
        "    mean_loss.reset_states()\n",
        "    \n",
        "    print(\"********* EPOCH :: {}\".format(e))\n",
        "\n",
        "    annot_list = get_shuffled_annots( 0 ,subset_size*step_limit )\n",
        "    offset = 0\n",
        "    steps = 0\n",
        "\n",
        "    if e%2==0 and e!=0:\n",
        "      l_rate = l_rate/5.0\n",
        "      optimizer.learning_rate = l_rate\n",
        "\n",
        "    while steps<step_limit:\n",
        "      \n",
        "      steps = steps + 1\n",
        "      images,images_caption = get_data( annot_list , offset , subset_size )\n",
        "      int_captions = convert_to_int( images_caption )\n",
        "      batches_img,batches_cap,nob = prep_batch(images,int_captions,batch_size)\n",
        "\n",
        "      offset = offset + subset_size\n",
        "\n",
        "      for i in range(nob):\n",
        "        \n",
        "        x_batch = batches_img[i]\n",
        "        y_batch = batches_cap[i]\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "          encoding_vec = encoder_model( x_batch , training=True)\n",
        "\n",
        "          hidden_state = encoding_vec\n",
        "\n",
        "          cost = 0\n",
        "          dec_input = tf.expand_dims( tf.convert_to_tensor( [ word_to_int['<start>'] ]*batch_size ) , -1 )\n",
        "          for t in range(1,y_batch.shape[1]):\n",
        "            pred_output,hidden_state = decoder_model( [dec_input , hidden_state] )\n",
        "            true_output = tf.expand_dims( y_batch[:,t] , -1 )\n",
        "\n",
        "            # We use teacher forcing , and give correct label for each timestep as input\n",
        "            dec_input = true_output\n",
        "\n",
        "            cost = cost + loss_fn( true_output , tf.squeeze(pred_output) )\n",
        "\n",
        "        \n",
        "        # You can avg. cost , but we don't as it will slow the learning\n",
        "        # avg_cost = cost/y_batch.shape[1]\n",
        "        mean_loss.update_state( cost )\n",
        "\n",
        "        trainable_variables = encoder_model.trainable_variables + decoder_model.trainable_variables\n",
        "        grads = tape.gradient( cost,trainable_variables )\n",
        "        optimizer.apply_gradients(zip(grads,trainable_variables))\n",
        "\n",
        "      print(\"Subset Loss is ==== > {} \".format(mean_loss.result()))\n",
        "\n",
        "    encoder_model.save('enc.h5')\n",
        "    decoder_model.save('dec.h5')  \n",
        "    print(\"Epoch Loss is ==== > {} \".format(mean_loss.result()))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  PS : Ignore the WARNING : it is bcz of varing time steps in input , which are expensive to parallelise.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBJ7ic16o366",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model.save('enc.h5')\n",
        "decoder_model.save('dec.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTczGfazjYHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#  taking a random range of images not encountered by model\n",
        "annot_list = get_shuffled_annots(100000,110000)\n",
        "# taking only 32 images data\n",
        "images,images_caption = get_data( annot_list , 0 , 32 )\n",
        "int_captions = convert_to_int( images_caption )\n",
        "batches_img,batches_cap,nob = prep_batch(images,int_captions,batch_size)\n",
        "\n",
        "x_batch = batches_img[0]\n",
        "y_batch = batches_cap[0]\n",
        "\n",
        "encoding_vec = encoder_model( x_batch , training=True)\n",
        "\n",
        "hidden_state = encoding_vec\n",
        "\n",
        "dec_input = tf.expand_dims( tf.convert_to_tensor( [ word_to_int['<start>'] ]*batch_size ) , -1 )\n",
        "sample_out = tf.expand_dims( tf.convert_to_tensor( [ word_to_int['<start>'] ]*batch_size ) , -1 )\n",
        "\n",
        "for t in range(1,y_batch.shape[1]):\n",
        "  pred_output,hidden_state = decoder_model( [dec_input , hidden_state] )\n",
        "\n",
        "  # We use Greedy sampling ie take most probable word\n",
        "  max_prob_out = tf.expand_dims( tf.squeeze( tf.cast( tf.argmax( pred_output , -1 ) , tf.int32 ) ) , -1 )\n",
        "  dec_input = max_prob_out\n",
        "\n",
        "  # concat best prediction of each timestep\n",
        "  sample_out = tf.concat([sample_out,max_prob_out],-1)\n",
        "\n",
        "\n",
        "print( sample_out.shape )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaBHNqO1C3OS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np_batch = x_batch.numpy()\n",
        "wcaps = convert_to_word( sample_out.numpy() )\n",
        "print( wcaps )\n",
        "\n",
        "for b in range(batch_size):\n",
        "  print( wcaps[b] )\n",
        "  show = plt.imshow((np_batch[b]+1.0)/2.0)\n",
        "  plt.show()\n",
        "\n",
        "\"\"\" \n",
        "  You can see even with only 50k samples subset we get interesting results.\n",
        "  Even if caption far from correct , it catches few nuances.\n",
        "  Feel Free to Train on Whole set of 400k images\n",
        "\"\"\"  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}