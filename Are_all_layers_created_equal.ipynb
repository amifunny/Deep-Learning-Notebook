{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Are_all_layers_created_equal.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJQ8+wH5nfX/ffhwNwOeFZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amifunny/Deep-Learning-Notebook/blob/master/Are_all_layers_created_equal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiGHJL6a612u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "  In this Notebook we check robustness of Deep Networks.\n",
        "\n",
        "  We check performance of model by probing effect of test performance by these two methods on each layers : -\n",
        "\n",
        "  --> Re-initialization\n",
        "  --> Randomization\n",
        "\n",
        "  Checkout Paper --> 'https://arxiv.org/pdf/1902.01996.pdf'\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b59_xQQM8yGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIMbBYNd8Bvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Our OLD SIMPLE HANDY MNIST DATASET\n",
        "\n",
        "raw_data,info = tfds.load('mnist',split=['train','test'],with_info=True)\n",
        "\n",
        "print(info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQEjhIse8KIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  The Usual Stuff\n",
        "train_data,test_data = raw_data[0],raw_data[1]\n",
        "\n",
        "def map_func(data):\n",
        "\n",
        "  img = tf.squeeze( ( tf.cast( data['image'] , tf.float32 )-127.5 )/127.5 )\n",
        "  label = tf.cast( data['label'] , tf.float32 )\n",
        "  return img,label\n",
        "\n",
        "train_data = train_data.map( map_func )\n",
        "\n",
        "train_data = train_data.shuffle(3000)\n",
        "train_batches = train_data.batch(256)\n",
        "\n",
        "test_data = test_data.map( map_func )\n",
        "\n",
        "test_data = test_data.shuffle(3000)\n",
        "test_batches = test_data.batch(256)\n",
        "\n",
        "for one_b in train_batches.take(1):\n",
        "  show = plt.imshow(one_b[0][0,:,:],cmap='gray')\n",
        "  plt.show()\n",
        "  print( one_b[0].shape )\n",
        "  print( one_b[1].shape )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE1w9fdw9Btl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct a simple FCN with 3 x 256 layer\n",
        "\n",
        "def get_model(shape_tuple):\n",
        "  inputs = tf.keras.layers.Input( shape_tuple )\n",
        "\n",
        "  out = tf.keras.layers.Flatten()(inputs)\n",
        "  out = tf.keras.layers.Dense(256,activation='relu')(out)\n",
        "  out = tf.keras.layers.Dense(256,activation='relu')(out)\n",
        "  out = tf.keras.layers.Dense(256,activation='relu')(out)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense( 10 , activation='softmax' )(out)\n",
        "\n",
        "  model = 0\n",
        "  model = tf.keras.Model( inputs , outputs )\n",
        "  print( model.summary() )\n",
        "  return model\n",
        "\n",
        "model = get_model( (28,28) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LAy1bmr-_fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store_weights(model):\n",
        "\n",
        "  initial_weights={}\n",
        "  trainable_layers = []\n",
        "\n",
        "  for idx,layer in enumerate( model.layers ):\n",
        "\n",
        "    if layer.trainable and layer.trainable_variables!=[]:\n",
        "      initial_weights[idx] = layer.get_weights()\n",
        "      trainable_layers.append( idx )\n",
        "\n",
        "  print( len( trainable_layers ) )\n",
        "  print( initial_weights.keys() ) \n",
        "\n",
        "  return trainable_layers,initial_weights\n",
        "\n",
        "trainable_layers,initial_weights = store_weights(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK2rUyq_gOi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#********************\n",
        "batch_size = 256\n",
        "ctr=0\n",
        "#********************\n",
        "\n",
        "def training(train_data,model, epochs=6 , l_rate = 0.0001 ):\n",
        "  \n",
        "  optimizer = tf.keras.optimizers.Adam( l_rate )\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "  with tf.device('/device:GPU:0'):\n",
        "\n",
        "    for e in range(epochs):\n",
        "      \n",
        "      print(\"********* EPOCH :: {}\".format(e))\n",
        "      ctr = 0\n",
        "      \n",
        "      train_data = train_data.shuffle(10000)\n",
        "      train_batches = train_data.batch(256)\n",
        "\n",
        "      for batch in train_batches:\n",
        "      \n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "          pred = model( batch[0] )\n",
        "          cost = loss_fn( batch[1] , pred )\n",
        "\n",
        "          if ctr%20==0:\n",
        "            print(\"Loss is === >  {}\".format(cost))\n",
        "\n",
        "          ctr = ctr+1\n",
        "          \n",
        "          grads = tape.gradient( cost,model.trainable_variables )\n",
        "          optimizer.apply_gradients(zip(grads,model.trainable_variables))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyFRWwQqCeCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training( train_data , model , 7 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWT_baYUjAQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_error( model , test_batches ):\n",
        "\n",
        "  acc = tf.keras.metrics.Accuracy()\n",
        "  acc.reset_states()\n",
        "\n",
        "  for batch in test_batches:\n",
        "     \n",
        "    pred = model( batch[0] )\n",
        "    pred_argmax = tf.argmax( pred , -1 )\n",
        "\n",
        "    acc.update_state( batch[1] , pred_argmax )\n",
        "\n",
        "  test_error =  1.0-acc.result()\n",
        "  print(\"Error is ==> {}\".format( test_error ) )\n",
        "\n",
        "  return test_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7zJiw1xpOt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_error( model , test_batches )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsk6_Dlh-SCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_robustness( layer , model , test_batches , initial_weights , robust_type):\n",
        "\n",
        "  randomizer = tf.keras.initializers.GlorotUniform()\n",
        "\n",
        "  # store the weights for again setting to model\n",
        "  weight = model.get_layer(index=layer).get_weights()\n",
        "\n",
        "  if robust_type=='randomize':\n",
        "    model.get_layer(index=layer).set_weights( [ randomizer( weight[0].shape ) , randomizer( weight[1].shape ) ] )\n",
        "  elif robust_type=='initialize':\n",
        "    model.get_layer(index=layer).set_weights( initial_weights[layer] )\n",
        "\n",
        "  error = test_error( model , test_batches )\n",
        "\n",
        "  # again set the true weight\n",
        "  model.get_layer(index=layer).set_weights( weight )\n",
        "\n",
        "  return error.numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRfxDm-Upsf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Checking Randomization Robustness :-- \")\n",
        "for i,layer_idx in enumerate(trainable_layers):\n",
        "\n",
        "  robust_error = check_robustness( layer_idx , model , test_batches , initial_weights , 'randomize' )\n",
        "  print(\"Layer {} :: --  {}\".format(i+1,robust_error))\n",
        "\n",
        "print(\"Checking Initialization Robustness :-- \")\n",
        "for i,layer_idx in enumerate(trainable_layers):\n",
        "\n",
        "  robust_error = check_robustness( layer_idx , model , test_batches , initial_weights , 'initialize' )\n",
        "  print(\"Layer {} :: --  {}\".format(i+1,robust_error))\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "  Oberserve the results , randomiztion completly destroy the ability of neural network. Which is obvious.\n",
        "\n",
        "  But Re-intialization show us pretty interesting Result.\n",
        "  \n",
        "  Layer 1 is what we call \"critical layer\". as Re -initializing it increase the error significantly.\n",
        "\n",
        "  But all other layers , when initialized on by one has no considerable effect on test_error.\n",
        "  These are called \"ambient layer\".\n",
        "\n",
        "  These Finding match the experiments done in paper.\n",
        "  We will find similar result of SOTA models like VGG ,ResNet ie some layers are \"critical\" and others are \"ambient\".\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmoqNqqo7_w8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Now Not so simple CIFAR-10 Dataset\n",
        "\n",
        "raw_data,info = tfds.load('cifar10',split=['train','test'],with_info=True)\n",
        "print(info)\n",
        "\n",
        "#  AGAIN The Usual Stuff\n",
        "cifar_train_data,cifar_test_data = raw_data[0],raw_data[1]\n",
        "\n",
        "def map_func(data):\n",
        "\n",
        "  img = tf.squeeze( ( tf.cast( data['image'] , tf.float32 )-127.5 )/127.5 )\n",
        "  label = tf.cast( data['label'] , tf.float32 )\n",
        "  return img,label\n",
        "\n",
        "cifar_train_data = cifar_train_data.map( map_func )\n",
        "\n",
        "cifar_train_data = cifar_train_data.shuffle(3000)\n",
        "cifar_train_batches = cifar_train_data.batch(256)\n",
        "\n",
        "cifar_test_data = cifar_test_data.map( map_func )\n",
        "\n",
        "cifar_test_data = cifar_test_data.shuffle(3000)\n",
        "cifar_test_batches = cifar_test_data.batch(256)\n",
        "\n",
        "for one_b in cifar_train_batches.take(1):\n",
        "  show = plt.imshow( one_b[0][0,:,:,:] )\n",
        "  plt.show()\n",
        "  print( one_b[0].shape )\n",
        "  print( one_b[1].shape )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzlyY0uM9rLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar_model = get_model( (32,32,3) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuOsdb2Y-kpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainable_layers , initial_weights = store_weights( cifar_model )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCwZHIdl9uK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training( cifar_train_data , cifar_model , 25 , 0.0001 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52HLWkTS-jMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_error( cifar_model , cifar_test_batches )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IKx_FFn_OkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Checking Randomization Robustness :-- \")\n",
        "for i,layer_idx in enumerate(trainable_layers):\n",
        "\n",
        "  robust_error = check_robustness( layer_idx , cifar_model , cifar_test_batches , initial_weights , 'randomize' )\n",
        "  print(\"Layer {} :: --  {}\".format(i+1,robust_error))\n",
        "\n",
        "print(\"Checking Initialization Robustness :-- \")\n",
        "for i,layer_idx in enumerate(trainable_layers):\n",
        "\n",
        "  robust_error = check_robustness( layer_idx , cifar_model , cifar_test_batches , initial_weights , 'initialize' )\n",
        "  print(\"Layer {} :: --  {}\".format(i+1,robust_error))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  CIFAR-10 is way more complex than MNIST and hece even without weights changing,\n",
        "  it gives ~0.45 error on test set.\n",
        "\n",
        "  Again our experiment match with the paper.\n",
        "  \n",
        "  Number of Critical layers increase with increase in complexity of task.\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}