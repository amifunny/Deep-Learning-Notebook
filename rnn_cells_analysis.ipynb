{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_cells_analysis.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnJvprlEkU//g3YvFc+QvV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amifunny/Deep-Learning-Notebook/blob/master/rnn_cells_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya34R6pzhIvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "AIM : TO CREATE TF ASSISTED HAND WRITTEN RNN_CELL , LSTM , GRU\n",
        "\n",
        "      AND COMPARE THEIR PERFORMANCE ON \"IMDB REVIEWs\" DATATSET\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkNuSfkggeOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ALL Imports Here\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qbwksn1gjFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  We Use Easy to feed  IMDB Dataset from TFDS( TENSORFLOW DATASETS ) at https://www.tensorflow.org/datasets/catalog/imdb_reviews\n",
        "\n",
        "  Split     No. of Entities\n",
        "  \n",
        "  test       25,000\n",
        "  train      25,000\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "raw_data,info = tfds.load( 'imdb_reviews' , split=['train','test'] , with_info=True )\n",
        "\n",
        "# see content map of datasets\n",
        "print(info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ot9PzzijqQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Pre - Trained GLove EMbeddings ( Quite a big File but save us from Embedding Training )\n",
        "!wget 'http://nlp.stanford.edu/data/glove.6B.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3yFiVBUkdGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzip the file\n",
        "!unzip 'glove.6B.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac2mp5JJj7qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Create a Hefty Dictionary \n",
        "  Matching each word to its Vector\n",
        "\n",
        "  ** We Use 100D file here. ( GLove offers 50d, 100d, 200d, & 300d vectors)\n",
        "  TO be clear 100D means each word is represented by 100 dimensional array\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "embed_dict = {}\n",
        "\n",
        "with open('/content/glove.6B.100d.txt') as file:\n",
        "\n",
        "  all_lines = file.readlines()\n",
        "  for each_line in all_lines:\n",
        "    \n",
        "    split_line = each_line.split()\n",
        "    word = split_line[0]\n",
        "    vector = np.array( split_line[1:] , dtype = np.float32 )\n",
        "    embed_dict[ word ] = vector\n",
        "\n",
        "  file.close()\n",
        "\n",
        "\n",
        "print(\"TXT FILE Line Example == > {}\".format(each_line) )\n",
        "print(\"Num of Words in Dictionary == > {}\".format( len(embed_dict) ))\n",
        "print(\"EXAMPLE :: \")\n",
        "print(\"Embedding of word '' {} '' is == > {}\".format( \"the\",embed_dict['the'] ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JgxM07Uo2EP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "embed_dim = 100\n",
        "hidden_dim = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cybFI77OioCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store test and train pointers in different Variable( actually they are iterators! )\n",
        "train_data,test_data = raw_data[0],raw_data[1]\n",
        "\n",
        "# \"map_fn\" is called every time daatset is used.\n",
        "def map_fn(text,label):\n",
        " \n",
        "  x = []\n",
        "  # remove all letters except alphanumerics and period. eg. \" don't \" become \" dont \"\n",
        "  cleaned_text = re.sub( r'[^a-zA-Z0-9. ]' , '' , text.numpy().decode('utf-8') )\n",
        "  # remove sticky periods\n",
        "  cleaned_text = re.sub( r'[.]' , ' .', cleaned_text )\n",
        "\n",
        "  for w in cleaned_text.lower().split():\n",
        "\n",
        "    try:\n",
        "      x.append( embed_dict[ w ] )\n",
        "    except:\n",
        "      # FOR <UNK> i.e Unknown Token\n",
        "      x.append( np.zeros(embed_dim) )\n",
        "      continue\n",
        "\n",
        "  y = tf.cast( label , tf.float32 )\n",
        "\n",
        "  return tf.convert_to_tensor(x , dtype=tf.float32 ),y\n",
        "\n",
        "# its just a \"wrapper\" for \"map_fn\" as map_fn is \"pythonic\" instead of \"tensorflow-ic!!!\"\n",
        "def map_fn_wrapper(data):\n",
        "  \"\"\"\n",
        "    Data is dict of \"label\" and \"text\"\n",
        "  \"\"\"\n",
        "\n",
        "  x,y = tf.py_function( map_fn , inp=[ data['text'],data['label'] ] , Tout=[ tf.float32,tf.float32 ] )\n",
        "  return x,y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pLVfsCb5zI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = train_data.shuffle( 25000 )\n",
        "train_data = train_data.map( map_fn_wrapper )\n",
        "train_data = train_data.padded_batch( batch_size , padded_shapes=( [None,embed_dim],[] ) , drop_remainder=True )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cshogFqO0h6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = test_data.shuffle( 25000 )\n",
        "test_data = test_data.map( map_fn_wrapper )\n",
        "test_data = test_data.padded_batch( batch_size , padded_shapes=( [None,embed_dim],[] ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxWYBAHj52fO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# see how a batch may look like\n",
        "for one_batch in train_data.take(1):\n",
        "  print( one_batch[0].shape )\n",
        "  print( one_batch[1].shape )\n",
        "  print( one_batch[1] )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0XZSwjqnwyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# see how a batch may look like\n",
        "for one_batch in train_data.take(1):\n",
        "  print( one_batch[0].shape )\n",
        "  print( one_batch[1].shape )\n",
        "  print( one_batch[1] )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3vZwFDgipm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "def training( model , epochs , train_batches , test_batches ):\n",
        "\n",
        "  avg_loss = tf.keras.metrics.Mean()\n",
        "  accuracy = tf.keras.metrics.Accuracy()\n",
        "\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    \n",
        "    for e in range(epochs):\n",
        "\n",
        "      print(\"EPOCH {}  ...  \".format(e))\n",
        "\n",
        "      avg_loss.reset_states()\n",
        "      accuracy.reset_states()\n",
        "\n",
        "      for train_batch in train_batches:\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "          pred = model( train_batch[0] )\n",
        "          loss = loss_fn( train_batch[1] , pred  )\n",
        "\n",
        "        grads = tape.gradient( loss , model.trainable_variables )\n",
        "        optimizer.apply_gradients( zip(grads,model.trainable_variables) )\n",
        "\n",
        "      for test_batch in test_batches:\n",
        "\n",
        "        pred = model( test_batch[0] )\n",
        "        pred_categ = tf.where(pred>0.5,1.0,0.0)\n",
        "        accuracy.update_state( test_batch[1] , pred_categ )\n",
        "\n",
        "      print( \"test_accuracy ==> {} ,  train_loss ==> {} \".format( accuracy.result(),avg_loss.result() ) )\n",
        "\n",
        "  return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5cYbawqLys7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SEQ_MODEL(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,cell,is_lstm=False):\n",
        "    super(SEQ_MODEL,self).__init__()\n",
        "    self.main_cell = cell\n",
        "    self.is_lstm = is_lstm\n",
        "\n",
        "    # this is bcz lstm has separate memory and hidden activations\n",
        "    if self.is_lstm:\n",
        "      self.hidden_state = [ tf.zeros( [batch_size,hidden_dim] ) , tf.zeros( [batch_size,hidden_dim] ) ]\n",
        "    else:\n",
        "      self.hidden_state = [ tf.zeros( [batch_size,hidden_dim] ) ]\n",
        "\n",
        "    \n",
        "  def call(self,inputs):\n",
        "    \"\"\"\n",
        "      inputs.shape => (batch, time_steps , hidden_dim)\n",
        "      prev_hidden_state.shape =>  (batch , hidden_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    seq_list = []\n",
        "\n",
        "    hidden_state = self.hidden_state\n",
        "\n",
        "    if inputs.shape[1] is None:\n",
        "      # For compilation of Model with Variable Shape \n",
        "      time_steps = 1\n",
        "    else:\n",
        "      time_steps = inputs.shape[1]\n",
        "\n",
        "    for each_step in range( time_steps ):\n",
        "\n",
        "      hidden_state = self.main_cell( inputs[:,each_step,:] , self.hidden_state )\n",
        "      seq_list.append( hidden_state[0] )\n",
        "\n",
        "    # final_output.shape ==> (batch , hidden_dim)\n",
        "    final_output = hidden_state[0]\n",
        "\n",
        "    # seq_output.shape ==> (batch, time_steps , hidden_dim)\n",
        "    seq_output = tf.stack( seq_list , axis=1 )\n",
        "\n",
        "    if self.is_lstm:\n",
        "      memory = hidden_state[1]\n",
        "      return final_output,seq_output,memory\n",
        "    else:\n",
        "      return final_output,seq_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-HZFsC07T-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CUSTOM RNN CELL\n",
        "class RNN_CELL(tf.keras.layers.Layer):\n",
        "  \n",
        "  def __init__(self,initializer,hidden_dim):\n",
        "    \n",
        "    super(RNN_CELL,self).__init__()\n",
        "\n",
        "    self.Waa = tf.Variable( initial_value=initializer( [hidden_dim,hidden_dim] ) , trainable=True )\n",
        "    self.Wax = tf.Variable( initial_value=initializer( [embed_dim,hidden_dim] ) , trainable=True )\n",
        "\n",
        "  def call(self,inputs,hidden_states):\n",
        "\n",
        "    prev_hidden_state = hidden_states[0]\n",
        "    hidden_state = tf.math.tanh( tf.matmul( prev_hidden_state , self.Waa ) + tf.matmul( inputs , self.Wax )  )\n",
        "    return [hidden_state]\n",
        "\n",
        "\n",
        "custom_cell = RNN_CELL( tf.keras.initializers.GlorotNormal() , hidden_dim )\n",
        "temp_hidden = custom_cell( tf.random.normal( [32,embed_dim] ) , [tf.zeros( [32,hidden_dim] )] )\n",
        "print( \"Hidden State Shape ==> {}\".format( temp_hidden[0].shape ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rty29iDFQp_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CUSTOM LSTM CELL\n",
        "class LSTM_CELL(tf.keras.layers.Layer):\n",
        "  \n",
        "  def __init__(self,initializer,hidden_dim):\n",
        "    \n",
        "    super(LSTM_CELL,self).__init__()\n",
        "\n",
        "    \"\"\"\n",
        "     these are stacked weights , to so we don't deal with too many Variables\n",
        "     Wax(embed_dim,hidden_dim) and Waa(hidden_dim,hidden_dim)---stacked to-- W(embed_dim+hidden_dim,hidden_dim)\n",
        "    \"\"\"\n",
        "    self.W_c = tf.Variable( initial_value=initializer( [embed_dim+hidden_dim,hidden_dim] ) )\n",
        "    self.W_u = tf.Variable( initial_value=initializer( [embed_dim+hidden_dim,hidden_dim] ) )\n",
        "    self.W_f = tf.Variable( initial_value=initializer( [embed_dim+hidden_dim,hidden_dim] ) )\n",
        "    self.W_o = tf.Variable( initial_value=initializer( [embed_dim+hidden_dim,hidden_dim] ) )\n",
        "\n",
        "  def call(self,inputs,hidden_states):\n",
        "\n",
        "    prev_hidden_state,prev_memory = hidden_states[0],hidden_states[1]\n",
        "\n",
        "    # stacked_input.shape ==> (batch_size,embed_dim+hidden_dim)\n",
        "    stacked_input = tf.concat( [inputs,prev_hidden_state] , 1 )\n",
        "    \n",
        "    memory = tf.math.tanh( tf.matmul( stacked_input , self.W_c ) )\n",
        "    update_gate = tf.math.sigmoid( tf.matmul( stacked_input , self.W_u ) )\n",
        "    forget_gate = tf.math.sigmoid( tf.matmul( stacked_input , self.W_f ) )\n",
        "    output_gate = tf.math.sigmoid( tf.matmul( stacked_input , self.W_o ) )\n",
        "    \n",
        "    memory = update_gate*memory + forget_gate*prev_memory\n",
        "    hidden_state = output_gate*tf.nn.tanh( memory )\n",
        "\n",
        "    \"\"\"\n",
        "    NOTICE : MEMORY CELL and HIDDEN State are different for LSTM\n",
        "    \"\"\"\n",
        "\n",
        "    return [hidden_state,memory]\n",
        "\n",
        "\n",
        "custom_cell = LSTM_CELL( tf.keras.initializers.GlorotNormal() , hidden_dim )\n",
        "temp_hidden = custom_cell( tf.random.normal( [32,embed_dim] ) , [tf.zeros( [32,hidden_dim] ) , tf.zeros( [32,hidden_dim] ) ] )\n",
        "print( \"Hidden State or Activation Shape ==> {}\".format(temp_hidden[0].shape) )\n",
        "print( \"Memory Shape ==> {}\".format(temp_hidden[1].shape) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuB_TxZo9_m2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CUSTOM GRU CELL\n",
        "class GRU_CELL(tf.keras.layers.Layer):\n",
        "  \n",
        "  def __init__(self,initializer,hidden_dim):\n",
        "    \n",
        "    super(GRU_CELL,self).__init__()\n",
        "\n",
        "    \"\"\"\n",
        "     these are stacked weights , to so we don't deal with too many Variables\n",
        "     Wax(embed_dim,hidden_dim) and Waa(hidden_dim,hidden_dim)---stacked to-- W(embed_dim+hidden_dim,hidden_dim)\n",
        "    \"\"\"\n",
        "    self.W_c = tf.Variable( initial_value=initializer( [embed_dim+hidden_dim,hidden_dim] ) )\n",
        "    self.W_u = tf.Variable( initial_value=initializer( [embed_dim+hidden_dim,hidden_dim] ) )\n",
        "    self.W_r = tf.Variable( initial_value=initializer( [embed_dim+hidden_dim,hidden_dim] ) )\n",
        "\n",
        "\n",
        "  def call(self,inputs,hidden_states):\n",
        "\n",
        "    prev_memory = hidden_states[0]\n",
        "\n",
        "    stacked_input = tf.concat( [inputs,prev_memory] , 1 )\n",
        "    rel_gate = tf.math.sigmoid( tf.matmul( stacked_input , self.W_r ) )\n",
        "\n",
        "    rel_input = tf.concat( [ inputs,rel_gate*prev_memory] , 1 )\n",
        "\n",
        "    memory = tf.math.tanh( tf.matmul( rel_input , self.W_c ) )\n",
        "    update_gate = tf.math.sigmoid( tf.matmul( stacked_input , self.W_u ) )\n",
        "    \n",
        "    memory = update_gate*memory + (1-update_gate)*prev_memory\n",
        "\n",
        "    \"\"\"\n",
        "    NOTICE : MEMORY CELL and HIDDEN State are same for GRU\n",
        "    You can denote memory in above code with 'hidden_state'\n",
        "    \"\"\"\n",
        "\n",
        "    return [memory]\n",
        "\n",
        "\n",
        "custom_cell = GRU_CELL( tf.keras.initializers.GlorotNormal() , hidden_dim )\n",
        "temp_hidden = custom_cell( tf.random.normal( [32,embed_dim] ) , [tf.zeros( [32,hidden_dim] )] )\n",
        "print( \"Hidden State Shape ==> {}\".format(temp_hidden[0].shape) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyaEeoK8R4aF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"Let the Training BEGINS\"\n",
        "\n",
        "# FIRST UP RNN MODEL\n",
        "rnn_model = SEQ_MODEL( RNN_CELL( tf.keras.initializers.GlorotNormal() , hidden_dim ) , is_lstm=False )\n",
        "\n",
        "inputs = tf.keras.layers.Input([None,embed_dim] , batch_size=batch_size)\n",
        "final_out,seq_out = rnn_model( inputs )\n",
        "out = tf.keras.layers.Dense(256,activation='relu')( final_out )\n",
        "outputs = tf.keras.layers.Dense(1,activation='sigmoid')( out )\n",
        "\n",
        "rnn_final_model = tf.keras.Model(inputs,outputs)\n",
        "rnn_final_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShU869fUOn5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training( rnn_final_model , 10 , train_data , test_data  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HHTK79NoQWZg",
        "colab": {}
      },
      "source": [
        "# SECOND UP LSTM MODEL\n",
        "lstm_model = SEQ_MODEL( LSTM_CELL( tf.keras.initializers.GlorotNormal() , hidden_dim ) , is_lstm=False )\n",
        "\n",
        "inputs = tf.keras.layers.Input([None,embed_dim] , batch_size=batch_size )\n",
        "final_out,seq_out = lstm_model( inputs )\n",
        "out = tf.keras.layers.Dense(256,activation='relu')( final_out )\n",
        "outputs = tf.keras.layers.Dense(1,activation='sigmoid')( out )\n",
        "\n",
        "lstm_final_model = tf.keras.Models(inputs,outputs)\n",
        "lstm_final_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1JG26dZ8QZN7",
        "colab": {}
      },
      "source": [
        "training( lstm_final_model , 10 , train_data , test_data  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KD3gf6uBQXIP",
        "colab": {}
      },
      "source": [
        "# THIRD UP GRU MODEL\n",
        "gru_model = SEQ_MODEL( GRU_CELL( tf.keras.initializers.GlorotNormal() , hidden_dim ) , is_lstm=False )\n",
        "\n",
        "inputs = tf.keras.layers.Input([180,embed_dim],batch_size=batch_size)\n",
        "final_out,seq_out = gru_model( inputs )\n",
        "outputs = tf.keras.layers.Dense(256)( final_out )\n",
        "\n",
        "gru_final_model = tf.keras.Model(inputs,outputs)\n",
        "gru_final_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A3h39TjUQZms",
        "colab": {}
      },
      "source": [
        "training( gru_final_model , 10 , train_data , test_data  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzSk8g0dPy2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SEQ_MODEL(tf.keras.Model):\n",
        "\n",
        "  \n",
        "  def __init__(self,cell,is_lstm=False):\n",
        "    super(SEQ_MODEL,self).__init__()\n",
        "    self.reccur_layer = Reccuring_layer( cell , is_lstm )\n",
        "\n",
        "    # layer to further compute upon 'recurr_layer' final output\n",
        "    self.dense1 = tf.keras.layers.Dense(hidden_dim)\n",
        "    self.dense0 = tf.keras.layers.Dense(1)\n",
        "    \n",
        "  def call(self,inputs):\n",
        "    \"\"\"\n",
        "      inputs.shape => (batch, time_steps , hidden_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    # for classification purpose ignore output at each time step\n",
        "    out,_ = self.reccur_layer(inputs)\n",
        "    out = self.dense1(out)\n",
        "    outputs = self.dense0(out)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}